\begingroup 
\renewcommand{\section}[2]{\anonsection{Список используемых источников}}
\begin{thebibliography}{00}

\bibitem{statmt-book}
    Koehn P. Statistical machine translation. – Cambridge University Press, 2009.
    
\bibitem{alexnet}
    Krizhevsky A., Sutskever I., Hinton G. E. Imagenet classification with deep convolutional neural networks //Advances in neural information processing systems. – 2012. – С. 1097-1105.
\bibitem{imagenet-human}
    He K. et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification //Proceedings of the IEEE international conference on computer vision. – 2015. – С. 1026-1034.
    
\bibitem{attn-all-need}
    Vaswani A. et al. Attention is all you need //Advances in neural information processing systems. – 2017. – С. 5998-6008.
    
\bibitem{attn-original}
    Bahdanau D., Cho K., Bengio Y. Neural machine translation by jointly learning to align and translate //arXiv preprint arXiv:1409.0473. – 2014.
    
\bibitem{lstm}
    Hochreiter S., Schmidhuber J. Long short-term memory //Neural computation. – 1997. – Т. 9. – №. 8. – С. 1735-1780.
    
\bibitem{gru}
    Chung J. et al. Empirical evaluation of gated recurrent neural networks on sequence modeling //arXiv preprint arXiv:1412.3555. – 2014.
    
\bibitem{encoder-decoder}
    Sutskever I., Vinyals O., Le Q. V. Sequence to sequence learning with neural networks //Advances in neural information processing systems. – 2014. – С. 3104-3112.
    
\bibitem{bert}
    Devlin J. et al. Bert: Pre-training of deep bidirectional transformers for language understanding //arXiv preprint arXiv:1810.04805. – 2018.
    
\bibitem{glue}
    Wang A. et al. Glue: A multi-task benchmark and analysis platform for natural language understanding //arXiv preprint arXiv:1804.07461. – 2018.
    
\bibitem{gpt2}
    Radford A. et al. Language models are unsupervised multitask learners //OpenAI Blog. – 2019. – Т. 1. – №. 8. – С. 9.
    
\bibitem{gpt1}
    Radford A. et al. Improving language understanding by generative pre-training //URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf. – 2018.
    
\bibitem{transfoxl}
    Dai Z. et al. Transformer-xl: Attentive language models beyond a fixed-length context //arXiv preprint arXiv:1901.02860. – 2019.
    
\bibitem{wikitext-prev}
    Rae J. W. et al. Fast parametric learning with activation memorization //arXiv preprint arXiv:1803.10049. – 2018.
    
\bibitem{text8-prev}
    Krause B. et al. Multiplicative LSTM for sequence modelling //arXiv preprint arXiv:1609.07959. – 2016.
    
\bibitem{1b-prev}
    Baevski A., Auli M. Adaptive input representations for neural language modeling //arXiv preprint arXiv:1809.10853. – 2018.

\bibitem{megatron-lm}
    Shoeybi M. et al. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism //arXiv preprint arXiv:1909.08053. – 2019.
    
\bibitem{roberta}
    Liu Y. et al. Roberta: A robustly optimized bert pretraining approach //arXiv preprint arXiv:1907.11692. – 2019.
    
\bibitem{mbert}
    Pires T., Schlinger E., Garrette D. How multilingual is Multilingual BERT? //arXiv preprint arXiv:1906.01502. – 2019.
    
\bibitem{xlm-roberta}
    Conneau A. et al. Unsupervised cross-lingual representation learning at scale //arXiv preprint arXiv:1911.02116. – 2019.
    
\bibitem{flaubert}
    Le H. et al. FlauBERT: Unsupervised Language Model Pre-training for French //arXiv preprint arXiv:1912.05372. – 2019.
    
\bibitem{bertje}
    de Vries W. et al. BERTje: A Dutch BERT Model //arXiv preprint arXiv:1912.09582. – 2019.
    
\bibitem{robbert}
    Delobelle P., Winters T., Berendt B. RobBERT: a dutch RoBERTa-based language model //arXiv preprint arXiv:2001.06286. – 2020.
    
\bibitem{germanbert}
    Risch J. et al. hpiDEDIS at GermEval 2019: Offensive Language Identification using a German BERT model //Preliminary proceedings of the 15th Conference on Natural Language Processing (KONVENS 2019). Erlangen, Germany: German Society for Computational Linguistics & Language Technology. – 2019. – С. 403-408.
    
\bibitem{arabert}
    Antoun W., Baly F., Hajj H. AraBERT: Transformer-based model for Arabic language understanding //arXiv preprint arXiv:2003.00104. – 2020.
    
\bibitem{rubert}
    Kuratov Y., Arkhipov M. Adaptation of deep bidirectional multilingual transformers for russian language //arXiv preprint arXiv:1905.07213. – 2019.
    
\bibitem{slavicbert}
    Arkhipov M. et al. Tuning multilingual transformers for language-specific named entity recognition //Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing. – 2019. – С. 89-93.
    
\bibitem{scibert}
    Beltagy I., Lo K., Cohan A. SciBERT: A pretrained language model for scientific text //Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). – 2019. – С. 3606-3611.
    
\bibitem{biobert}
    Lee J. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining //Bioinformatics. – 2020. – Т. 36. – №. 4. – С. 1234-1240.
    
\bibitem{distilbert}
    Sanh V. et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter //arXiv preprint arXiv:1910.01108. – 2019.
    
\bibitem{know-dist}
    Hinton G., Vinyals O., Dean J. Distilling the knowledge in a neural network //arXiv preprint arXiv:1503.02531. – 2015.
    

\end{thebibliography}
\endgroup

\clearpage
