\section{Обзор современных подходов к использованию нейросетевых методов при анализе текста} \label{literature}

\par Нейросетевые методы вообще, и в применении для анализа текста на естественном языке в частности, являются горячей темой среди исследователей по всему миру. Каждый день публикуются десятки статей на данную тематику, а исследовательские группы, занимающиеся разработкой новых подходов к применению нейронных сетей, есть во всех крупных компаниях и универститетах. В данном разделе будет приведен обзор последних достижений в этой сфере.

\par В первую очередь, необходимо упомянуть работу~\cite{attn-all-need}, которая в свою очередь является глубоким развитием идей механизма внимания(Attention), впервые представленном в~\cite{attn-original}. Авторы предложили полностью избавиться от реккурентных компонентов в нейронной сети в угоду механизму трансформера. В отличии от реккурентных блоков(таких как LSTM~\cite{lstm} и GRU~\cite{gru}), трансформер использует меньше операций и его можно параллелизовать на несколько устройств с независимой памятью(что полезно в случае работы с большим количеством тренировочных данных). Схематично структура трансформера изображена на рисунке \ref{transformer}.
\addimghere{transformer}{0.85}{Архитектура сети Трансформер}{transformer}

\par Изначально, данная архитектура сети проектировалась для задач машинного перевода, поэтому она следует стандартной концепции кодировщик-декодировщик~\cite{encoder-decoder} (Encoder-Decoder). Однако, вскоре после публикации оригинальной статьи, исследователи успешно применили Трансформер для реализации переноса обучения (transfer learning) в задачах обработки текста. Авторы из исследовательского подразделения корпорации Google предложили архитектуру "двунаправленного представления кодировщика из трансформера"(Bidirectional Encoder Representations from Transformers - далее BERT). Представленная ими модель предварительно обучается на задаче маскированного языкового моделирования и предсказания следующего предложения. Первая из этих задач представляет собой измененный вариант задачи языкового моделирования, в котором необходимо предсказать 15\% замаскированных токенов из предложения. Вторая задача отдаленно напоминает задачу логического вывода по тексту и формулируется следующим образом: дано два предложения, необходимо сделать вывод о том идут-ли они друг за другом в тексте или нет. Положительные примеры для этой задачи берутся из двух последовательных предложений текста, отрицательные - случайно выбираются два предложения, не стоящие друг за другом. Для предобучения на таких задачах авторы статьи использовали объединенный датасет из текстов статей английской Википедии(Wikipedia) и корпус книжных текстов(Toronto BookCorpus). Далее, после предобучения, модель дополнительно дообучается на конечной задаче, будь то классификация, вопросно-ответный поиск и т.д. На таких задачах, а именно на задачах из датасета GLUE Benchmark~\cite{glue}, модель BERT показала себя значительно лучше, чем все предыдущие подходы, основанные на использовании реккурентных сетей. Данные представлены в таблице \ref{glue-scores}.

\begin{table}[H]
  \caption{Сводная таблица метрик качества на датасетах GLUE Benchmark}\label{glue-scores}
  \begin{tabular}{|c|c|c|c|}
  \hline
  Задача & BiLSTM+ELMo+Attn & BERT_{BASE} & BERT_{LARGE} \\
  \hline
  MNLI-(m/mm) & 76.4/76.1 & 84.6/83.4 & 86.7/85.9 \\
  QQP         & 64.8      & 71.2      & 72.1      \\
  QNLI        & 79.8      & 90.5      & 92.7      \\
  SST-2       & 90.4      & 93.5      & 94.9      \\
  CoLA        & 36.0      & 52.1      & 60.5      \\
  STS-B       & 73.3      & 85.8      & 86.5      \\
  MRPC        & 84.9      & 88.9      & 89.3      \\
  RTE         & 56.8      & 66.4      & 70.1      \\
  \hline
  Среднее     & 71.0      & 79.6      & 82.1      \\
  \hline 
  \end{tabular}
\end{table}

\par С точки зрения архитектуры нейронной сети, BERT использует нескольких слоев, каждый из которых представляет собой вариацию блока Трансформера из оригинальной статьи, состоящего из Кодировщика. Количество слоев,"голов" (head) и размер вектора внутреннего представления токенов варьируется в зависимости от варианта модели: $BERT_{BASE}$ состоит из 12 слоев, каждый из которых включает в себя 12 голов и оперирует в пространстве размерности 768; $BERT_{LARGE}$, в свою очередь, состоит из 24 слоев, по 16 голов на каждый и размерность пространства равна 1024. Как видно из таблицы выше, $BERT_{LARGE}$ показывает себя немного лучше чем базовая версия. Однако размер этих моделей отличается значительно: 110 миллионов параметров в $BERT_{BASE}$, против 340 миллионов в $BERT_{LARGE}$. Из-за этого тренировка и использование модели LARGE крайне затруднительно и распространие получила базовая модель.

%%--
%\addimg{full_virt}{0.35}{Архитектура полной виртуализации}{full-virt}

\clearpage
