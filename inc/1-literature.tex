\section{Обзор современных подходов к использованию нейросетевых методов при анализе текста} \label{literature}

\par Нейросетевые методы вообще, и в применении для анализа текста на естественном языке в частности, являются горячей темой среди исследователей по всему миру. Каждый день публикуются десятки статей на данную тематику, а исследовательские группы, занимающиеся разработкой новых подходов к применению нейронных сетей, есть во всех крупных компаниях и универститетах. В данном разделе будет приведен обзор последних достижений в этой сфере.

\par В первую очередь, необходимо упомянуть работу~\cite{attn-all-need}, которая в свою очередь является глубоким развитием идей механизма внимания(Attention), впервые представленном в~\cite{attn-original}. Авторы предложили полностью избавиться от реккурентных компонентов в нейронной сети в угоду механизму трансформера. В отличии от реккурентных блоков(таких как LSTM~\cite{lstm} и GRU~\cite{gru}), трансформер использует меньше операций и его можно параллелизовать на несколько устройств с независимой памятью(что полезно в случае работы с большим количеством тренировочных данных). Схематично структура трансформера изображена на рисунке \ref{transformer}.
\addimghere{transformer}{0.85}{Архитектура сети Трансформер}{transformer}

\par Изначально, данная архитектура сети проектировалась для задач машинного перевода, поэтому она следует стандартной концепции кодировщик-декодировщик~\cite{encoder-decoder} (Encoder-Decoder). Однако, вскоре после публикации оригинальной статьи, исследователи успешно применили Трансформер для реализации переноса обучения (transfer learning) в задачах обработки текста. Авторы из исследовательского подразделения корпорации Google предложили~\cite{bert} архитектуру "двунаправленного представления кодировщика из трансформера"(Bidirectional Encoder Representations from Transformers - далее BERT). Представленная ими модель предварительно обучается на задаче маскированного языкового моделирования и предсказания следующего предложения. Первая из этих задач представляет собой измененный вариант задачи языкового моделирования, в котором необходимо предсказать 15\% замаскированных токенов из предложения. Вторая задача отдаленно напоминает задачу логического вывода по тексту и формулируется следующим образом: дано два предложения, необходимо сделать вывод о том идут-ли они друг за другом в тексте или нет. Положительные примеры для этой задачи берутся из двух последовательных предложений текста, отрицательные - случайно выбираются два предложения, не стоящие друг за другом. Для предобучения на таких задачах авторы статьи использовали объединенный датасет из текстов статей английской Википедии(Wikipedia) и корпус книжных текстов(Toronto BookCorpus). Далее, после предобучения, модель дополнительно дообучается на конечной задаче, будь то классификация, вопросно-ответный поиск и т.д. На таких задачах, а именно на задачах из датасета GLUE Benchmark~\cite{glue}, модель BERT показала себя значительно лучше, чем все предыдущие подходы, основанные на использовании реккурентных сетей. Данные представлены в таблице \ref{glue-scores}.

\begin{table}[H]
  \caption{Сводная таблица метрик качества на датасетах GLUE Benchmark}\label{glue-scores}
  \begin{tabular}{|c|c|c|c|}
  \hline
  Задача & BiLSTM+ELMo+Attn & BERT_{BASE} & BERT_{LARGE} \\
  \hline
  MNLI-(m/mm) & 76.4/76.1 & 84.6/83.4 & 86.7/85.9 \\
  QQP         & 64.8      & 71.2      & 72.1      \\
  QNLI        & 79.8      & 90.5      & 92.7      \\
  SST-2       & 90.4      & 93.5      & 94.9      \\
  CoLA        & 36.0      & 52.1      & 60.5      \\
  STS-B       & 73.3      & 85.8      & 86.5      \\
  MRPC        & 84.9      & 88.9      & 89.3      \\
  RTE         & 56.8      & 66.4      & 70.1      \\
  \hline
  Среднее     & 71.0      & 79.6      & 82.1      \\
  \hline 
  \end{tabular}
\end{table}

\par С точки зрения архитектуры нейронной сети, BERT использует нескольких слоев, каждый из которых представляет собой вариацию блока Трансформера из оригинальной статьи, состоящего из Кодировщика. Количество слоев,"голов" (head) и размер вектора внутреннего представления токенов варьируется в зависимости от варианта модели: $BERT_{BASE}$ состоит из 12 слоев, каждый из которых включает в себя 12 голов и оперирует в пространстве размерности 768; $BERT_{LARGE}$, в свою очередь, состоит из 24 слоев, по 16 голов на каждый и размерность пространства равна 1024. Как видно из таблицы выше, $BERT_{LARGE}$ показывает себя немного лучше чем базовая версия. Однако размер этих моделей отличается значительно: 110 миллионов параметров в $BERT_{BASE}$, против 340 миллионов в $BERT_{LARGE}$. Из-за этого тренировка и использование модели LARGE крайне затруднительно и распространение получила базовая модель.
\subsection{Языковое моделирование}
\par Так же, архитектура Трансформера и её компоненты успешно применяются в задаче языкового моделирования. Начиная обозревать литературу по этой задаче нельзя не упомянуть работу~\cite{gpt2}. Авторы использовали оригинальную архитектуру из статьи~\cite{gpt1}, но значительно увеличили размеры корпуса текстов, на котором модель обучается. Данный корпус представлял собой набор текстов с веб-страниц, на которые имеется ссылка с веб-сайта Reddit\footnote{https://www.reddit.com/}. Притом выбирались только те ссылки, которые имели показатель "кармы"(аналог количества лайков) не менее 3-х. Таким образом был собран корпус текстов, объемом 40 гигабайт. Далее модель обучалась на стандартной задаче языкового моделирования. В результате анализа обученной модели, авторы пришли к выводу, что хоть она и не дообучалась на конечных задачах, таких как анализ сентимента, вопросно-ответный поиск и даже машинный перевод, она вполне может с ними справляться, при условии если они сформулированы через задачу задачу языкового моделирования.
См. пример в таблице \ref{gpt-qa}.

\begin{table}[H]
  \caption{Пример работы модели GPT-2 на задаче ответа на вопросы}\label{gpt-qa}
  \begin{tabular}{|c|c|}
  \hline
  Анализируемый текст & Сгенерированный ответ \\
  \hline
  Who wrote the book the origin of species? & Charles Darwin \\
  Nuclear power plant that blew up in russia? & Chernobyl \\
  Panda is a national animal of which country? & China \\
  \hline 
  \end{tabular}
\end{table}

\par Помимо вышеупомянутой работы, стоит отметить статью~\cite{transfoxl}. В ней авторами был описан метод решения одной из фундаментальных проблем архитектуры Трансформер - ограниченность длины анализируемого предложения. Представлен механизм позиционально-относительного кодирования. Он позволяет последовательно анализовать контексты большой длинны(больше стандартных 512 токенов). Это позволило применять модель для языкового моделирования в стандартных условиях для классических датасетов - WikiText103, text8, enwiki8 и 1 Billion Word. На всех датасетах модель показала значительное улучшения метрик(см. таблицу \ref{transfoxl-metrics}).

\begin{table}[H]
  \caption{Значения метрик TransformerXL и других моделей в задаче языкового моделирования. Для датасетов WikiText103 и 1 Billion Word приведены показатели перплексии, для датасетов text8 и enwiki8 - отношения числа бит к числу символов. В обоих случаях лучшим значением является наименьшее}\label{transfoxl-metrics}
  \begin{tabular}{|c|c|c|}
  \hline
  Датасет & TransformerXL & Предыдущий лучший результат \\
  \hline
  WikiText103    & 18.3 & 29.9~\cite{wikitext-prev} \\
  text8          & 1.08 & 1.27~\cite{text8-prev} \\
  enwiki8        & 0.99 & 1.24~\cite{text8-prev} \\
  1 Billion Word & 21.8 & 23.7~\cite{1b-prev} \\
  \hline 
  \end{tabular}
\end{table}
Тем не менее, из недостатков стоит отметить значительно возвросший размер моделей, по сравнению с реккурентными нейронными сетями. Так, для датасета text8 авторы использовли модель с общим количеством параметров 277 миллионов, тогда как предыдущий лучший результат был получен с помощью модели с 45 миллионами параметров.

\par В задачах, когда фактический размер модели не является ограничением, архитектура Трансформера позволяет добиваться лучших результатов простым увеличением "емкости" модели: увлечением числа слоев, числа "голов" в блоках self-attention, увеличением размера внутреннего векторного пространства. Отличный пример такого увеличения представлен в работе~\cite{megatron-lm}. Исследователи из компании Nvidia представили архитектуру Megatron-LM для решения задачи языкового моделирования. В работе представлены несколько вариантов моделей с такой архитектурой, с числом параметров от 1.2 миллиардов до 8.3. Ввиду технических ограничений на объем памяти в графических ускорителях(GPU), использующихся при обучении моделей, большинство из представленных моделей невозможно обучать целиком на одном ускорителе, поскольку помимо самих весов модели, для обучения необходимо хранить градиенты и показатели momentum. Для решения этой задачи авторы предложили разместить разные слои модели на разных ускорителях(т.н подход модельного параллелизма). Это позволило задействовать 8 ускорителей с 16 гигабайтами встроенной памяти для обучения одной копии модели. Помимо этого был реализован параллелизм на уровне данных, который позволил работать с мини-батчем большого размера, что положительно сказалось на скорости сходимости модели. В результате, самая большая модель с 8.3 миллиардами параметров обучалась суммарно на 512 графических ускорителях(использовался супер-компьютер из 64 узлов, каждый из которых содержал 8 ускорителей Nvidia V100). На датасете WikiText-103 модель показала перплексию в 10.3.

\subsection{Предтренировка моделей}
\par Статья~\cite{bert} задала новый тренд в методах анализа текста на естественно языке. Сначала модель обучается на большом количестве данных на задачах само-обучения(self-supervision). Таким образом модель в неявном виде кодирует внутри себя информацию о морфологии, синтаксисе и семантике языка(или нескольких языков, в случае мультиязычной модели). Далее, предобученная таким образом модель достраивается небольшим однослойным перцептроном(в некоторых случаях двух-трех слойным), и дообучается на относительно небольшом количестве данных и значительно меньшей скорости обучения на конечной задаче обучения с учителем(supervised learning). Это позволяет сократить время обучения и переиспользовать вычислительные ресурсы.

\par Появилось большое количество исследований, которые в той или иной мере улучшаю концепцию BERT-а в сторону оптимизации размера модели и/или улучшения метрик на конечных задачах. Среди таких работ стоит в первую очередь отметить~\cite{roberta}. Группа авторов из корпорации Facebook и Университета Вашингтона представили архитектуру RoBERTa(Robustly optimized BERT pretraining approach - "надежно оптимизированный подход к предобучению BERT-а"). Был проанализирован подход к обучению BERT-а и предложены улучшения. Во-первых, для задачи masked language modelling было решено маскировать токены случайным образом на каждом шаге обучения, вместо предварительной обработки всего датасета. Таким образом, модель каждый раз видит новые данные, и ей ни раз не попадается одинаково маскированные предложения. Во-вторых, предобучение велось без задачи Next Sentence Prediction, поскольку в результате экспериментов было обнаружено, что простое соединение нескольких предложений в один пример показывает лучшие метрики на конечных задачах, чем соединение и одновременное обучение на NSP-задаче. В-третьих, предобучение модели велось с значительно большим мини-батчем, притом сохраняя общее количество проходов по датасету. Это удалось достичь с помощью техники аккумуляции градиента и параллелизма на уровне данных. Помимо этого, авторы предтренировывали модели на значительно большем датасете(160 гигабайт против 13 гигабайт в оригинальном BERT). В результате данных улучшений удалось добиться значительного улучшения качества работы модели на конечных задачах (см. значения метрик в таблице \ref{roberta-metrics}. Притом сама архитектура модели осталась неизменной. 

\begin{table}[H]
  \caption{Значения метрик для моделей RoBERTa и BERT на различных датасетах}\label{roberta-metrics}
  \begin{tabular}{|c|c|c|с|c|c|}
  \hline
  Модель & Объем Датасета & Мини-Батч & SQuAD(v1.1/v2) & MNLI-m & SST-2 \\
  \hline
  RoBERTa & 16 ГБ & 8000 & 93.6/87.3 & 89.0 & 95.3\\
  RoBERTa & 160 ГБ & 8000 & 94.6/89.4 & 90.2 & 96.4 \\
  BERT & 13 ГБ & 256 & 90.9/81.8 & 86.6 &  93.7 \\
  \hline 
  \end{tabular}
\end{table}

\par Вышеописанная модель была представлена только для английского языка, т.к для предобучения собирались тексты на английском языке. Однако авторы BERT, помимо английского языка, представили мультиязычную модель для 101 языка, обученную на конкатенации текстов статей с веб-сайта Википедия. Для такой мультиязычной модели применим подход, называемый кросс-языковым переносом без дообучения(Zero-shot cross-lingual transfer)~\cite{mbert}. Суть данного подхода заключается в том, что мультиязычная модель дообучается на конечной задачи для одного языка, а применяться может на другом языке из своего словаря. Этот подход показывает результаты хуже, чем если дообучать модель сразу на другом языке, однако неоспоримым преимуществом является то, что с его помощью можно решать задачи анализа текстов на не-распространенных языках, для которых нет необходимых датасетов. Существует работа~\cite{xlm-roberta}, посвященная адаптации архитектуры RoBERTa для работы в мультиязычном варианте. Авторы следуют подходу из оригинальной статьи, в то же время значительно увечиливая датасет. Помимо данных из Википедии используется предобработанные тексты из датасета CommonCrawl. Суммарный объем датасета составил 2 Терабайта. В результате удалось добиться значительного улучшения качества работы модели на нераспространенных языках, по сравнению с мультиязычной версией BERT(см. результаты в таблице \ref{xlm-roberta})

\begin{table}[H]
  \caption{Значения метрик для моделей XLM-RoBERTa и BERT на задаче мультиязычного выделения ответа по контексту (датасет MLQA)}\label{xlm-roberta}
  \begin{tabular}{|c|c|c|с|}
  \hline
  Язык & Multilingual BERT & XLM-RoBERTa_{base} & XLM-RoBERTa\\
  \hline
  Английский & 77.7/65.2 & 77.1/64.6 & 80.6/67.8 \\
  Испанский  & 64.3/46.6 & 67.4/49.6 & 74.1/56.0 \\
  Немецкий   & 57.9/44.3 & 60.9/46.7 & 68.5/53.6 \\
  Арабский   & 45.7/29.8 & 54.9/36.6 & 63.1/43.5 \\
  Хинди      & 43.8/29.7 & 59.4/42.9 & 69.2/51.6 \\
  Вьетнамский& 57.1/38.6 & 64.5/44.7 & 71.3/50.9 \\
  Китайский  & 57.5/37.3 & 61.8/39.3 & 68.0/45.4 \\
  \hline
  Среднее    & 57.7/41.6 & 63.7/46.3 & 70.7/52.7 \\
  \hline
  \end{tabular}
\end{table}

\par Одновременно с мультиязычными моделями, развиваются и моноязычные. Так, существуют вариант BERT для текстов на французском языке - FlauBERT~\cite{flaubert}, для голландского языка - BERTje~\cite{bertje} и RobBERT~\cite{robbert}(вариант RoBERTa для голландского языка). BERT для немецкого языка~\cite{germanbert} был предтренирован для того, чтобы в дальнейшем дообучить его на задаче распознавания нецензурной лексики и показал значительное улучшение метрик на данной задаче. Для арабского языка существует адаптация под названием AraBERT~\cite{arabert}. Отдельно стоит отметить семейство моделей для славянской языковой семьи, разработанных в МФТИ. Это RuBERT для русского языка~\cite{rubert}, и SlavicBERT~\cite{slavicbert} для чешского, польского, болгарского и русского языков. Крайне интересным является подход, предложенный в RuBERT: авторы использовали веса мультиязычной версии BERT, заменили все не-кириллические токены на кириллические и таким образом значительно сократили время предтренировки.

\par Отдельным направлением можно считать предобучение моделей семейства BERT на текстах определенной тематики. По данному направлению есть две заметные работы. Первая - статья о модели SciBERT~\cite{scibert}. Авторы использовали стандартную архитектуру BERT-а, увеличили словарь, используя специфичные термины для научных статей и предобучили модель на 1.12 миллиона научных статей из базы Semantic Scholar. В результате, модель SciBERT опережает стандартный BERT по качеству предсказаний на конечных задачах, связанных с анализом научных текстов, в т.ч разметке сущностей в медицинских и биологических статьях, PICO-разметке и разметке зависимостей. Вторая статья - о модели BioBERT~\cite{biobert}. В данной работе исследователи так-же использовали оригинальную архитектуру BERT и предобучали его на датасете биомедицинских текстов. Датасет был собран из аннотаций научных статей из базы PubMed и полных текстов статей из базы PMC. Предобучение заняло 23 дня на 8 графических ускорителях Nvidia Tesla V100. В результате данная модель показала прирост качества на конечных задачах анализа биомедицинских текстов. Авторы заявляют, что на задаче распознавания именованных сущностей в биомедицинских текстах прирост составил 0.62\% по F-мере, на задаче извлечении связей - 2.8\% по F-мере и на задаче вопросно-ответного поиска по контексту - на 12.24\% по метрике MRR.

\par Помимо улучшения качества предсказаний, можно выделить несколько работ о улучшении производительности BERT-моделей. Для начала обратим внимание на работу~\cite{distilbert} о дистиляции модели BERT. В данной статье используется техника дистиляции знаний~\cite{know-dist} из модели(Knoweledge Distillation). Суть данной техники в следующем - беруться две модели - оригинальная, с большим количеством параметров("Учитель") и новая, не предобученная модель с значительно меньшим числом параметров("Ученик"). Ученик обучается на оригинальной задаче учителя(в данном случае - MLM) с дополнительной функцией ошибки, которая рассчитывает схожесть выходного распределения ученика и учителя. В данной работе в качестве такой функции ошибки используется Дивергенция Кульбака-Лейблера: $$D_{KL}(P\parallel Q)=\sum \limits _{i=1}^{n}p_{i}\log {\frac {p_{i}}{q_{i}}}$$ Где P и Q - распределение вероятностей слов на месте маскированных токенов для модели-учителя и модели-студента соответственно.
Альтернативой данной функции ошибок может служить косинусное расстояние между векторами: $$cosine~distance=\cos(\theta )={A\cdot B \over \|A\|\|B\|}={\frac  {\sum \limits _{{i=1}}^{{n}}{A_{i}\times B_{i}}}{{\sqrt  {\sum \limits _{{i=1}}^{{n}}{(A_{i})^{2}}}}\times {\sqrt  {\sum \limits _{{i=1}}^{{n}}{(B_{i})^{2}}}}}}$$ Где A и B - выходные векторы модели-учителя и модели-студента соответственно.
Авторы статьи провели такую дистилляцию на максимально-близком к оригиналу датасете(тексты статей из Википедии + тексты книг из Toronto BookCorpus). В результате была получена модель DistilBERT, которая ожидаемо отстает в качестве предсказаний на конечных задачах из датасета GLUE Benchmark - в среднем на 3\%, однако имеет значительно меньше параметров чем оригинальный BERT - 66 миллионов против 110. Так же авторы отмечают меньшее время работы модели.

\begin{itemize}
    \item Добавить про ALBERT и квантизацию
    \item Добавить про Elmo и Flair
\end{itemize}

%%--
%\addimg{full_virt}{0.35}{Архитектура полной виртуализации}{full-virt}

\clearpage
